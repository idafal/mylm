--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Name1, Name2 (subtitle)" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output: # 3rd letter intentation hierarchy
 # html_document:
 #    toc: true
 #  toc_float: true
#    toc_depth: 2
   pdf_document:
    toc: false
    toc_depth: 2

---
```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Introduction

In this compulsary exercise we are going to make an r package, called "mylm", with some of the same functionality as found in the already existing r package `lm`. We are going to test our implementations on the

# Part 1

## a)

The aim of part 1 is to get an overview of the dataset before making the linear regressions. The following diagram shows some diagnostic polts based on the data set from Canada in the car-library, containing information about `wages`, `education`, `age`, `sex` and `language`. 

```{r, echo = FALSE}
library(GGally)
#install.packages("car")
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]
ggpairs(SLID)
```

In the later exercises we are going to perform regression using `wages` as response. The properties that seem to be most correlated with the `wages` is `age` and `education`, with numerical correlation value of 0.36 and 0.306 respectively. On the scatter plots of `wages` versus `age` and `wages` versus `education`, it can be seen that among those with highest wages there are few with low age and low education. 

On the other hand, the wage level does not seem to be related to the `language` of the observed people. It also shows little correlation with `sex`, but the average wage is slightly higher for the males than for the females. 

If we want to perform a multiple linear regression analysis to predict wages based on some of the other variables we have to assume that relationship between wages and the variebles is in fact linear. Also, the residuals are assumed to be normally distributed and homoscedastic, and the explanatory covariates are assumed to be uncorrelated.

Based on the diagnostic plots, none of the explanatory variables seem to be highly correlated. However, for instance `education` and `age` correlates slightly, with coefficient of correlation of -0.106. 

#Part 2

## a) 

```{r, echo = FALSE}
mylm <- function(formula, data = list(), contrasts = NULL, ...){
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")
  n = nrow(X)
  p = ncol(X)
  k = p-1

  # Add code here to calculate coefficients, residuals, fitted values, etc...
  # and store the results in the list est
  beta_hat = solve(t(X)%*%X)%*%t(X)%*%y
  sigma_sq_hat = (1/(n-p))*t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  cov_beta_hat = drop(sigma_sq_hat)*solve(t(X)%*%X)
  est <- list(terms = terms, model = mf)
  est$coefficients <- beta_hat
  est$cov <- cov_beta_hat
  
  est$X <- X
  est$y <- y

  # Store call and formula used
  est$call <- match.call()
  est$formula <- formula

  df = n-p
  SSE = t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  SST = t(y)%*%(diag(n)-drop(1/n)*rep(1, n)%*%t(rep(1, n)))%*%y
  R2 = 1-SSE/SST
  X2 = ((SST-SSE))/(SSE/(n-p))
  pX2 = pchisq(X2, k)
  # What is the relationship between the X2-statistic and the z-statistic?
  # Kanskje "Testing one regression coefficient" i modul 2
  # Find the critical value for both tests
  vec = c(SSE, SST, R2, X2, pX2, n, p, df)
  est$vec = vec

  # Set class name. This is very important!
  class(est) <- 'mylm'

  # Return the object with all results
  return(est)
}

print.mylm <- function(object, ...){
  # Code here is used when print(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("Coefficients: ")
  for (i in 1:(length(names))){
    cat("\n")
    cat(names[i])
    cat(": ")
    cat(object$coefficients[i])
  }
  cat("\n")
}

summary.mylm <- function(object, ...){
  # Code here is used when summary(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("R squared: ")
  cat(object$vec[3])
  cat("\n")
  cat("Coefficients:\n")
  SD = vector(length = object$vec[7])
  for(i in 1:object$vec[7]){
    SD[i] = sqrt(object$cov[i,i])
  }
  Estimates = object$coefficients
  ztest = Estimates/SD
  pvalue = 2*pnorm(-abs(ztest))
  table = data.frame(Estimates, SD, ztest, pvalue)
  rownames = vector(length = object$vec[7])
  for(i in 1:length(names)){
    rownames[i] = names[i]
  }
  row.names(table) = rownames
  format(table, digits = 4)
}


```

In order to estimate the regression coefficients, $\mathbf{\hat{\beta}}$, we have used the matrix formula

${\bf{\hat{\beta}}} = (X^TX)^{-1}X^TY .$

The following printouts gives the estimated intercept and regression coefficient from the linear regression with `wages` as response and `education` as predictor using `mylm` and `lm` respectively.

```{r, echo=FALSE}
model1 <- mylm(wages ~ education, data = SLID)
print(model1)

model1b <- lm(wages ~ education, data = SLID)
print(model1b)

```

This shows that the two functions calculates equal values, indicating that our function works as it was meant to in this case. 

## b)

We have estimated the variance as

$\hat{\sigma}^2 = \frac{1}{n-p} (Y -X\hat{\beta})^T(Y-X\hat{\beta}),$

and further found the covariance matrix as

$\text{Cov} = \hat{\sigma}^2 X^T X.$

The standard deviation for the estimated coefficients are set to the corresponding square roots of the diagonal element of the covariance matrix. 

The z-statistic for the z-test has the value

$z = \frac{\hat{\beta} - \beta_0}{SD} = \frac{\hat{\beta}}{SD},$

where $\beta_0$ is the beta-value of the the null hypothesis, in this case $\beta_0 = 0$. 

The p-value is calculated as $2 \times \text{`pnorm`}(-|z|)$, where "pnorm" is a function in R which returns the p-value corresponding to the value of its input statistic. We are interested in the p-value corresponding to the two-sided test, so therefore we are multiplying the value by two. 

$R^2$ is calculated as 

$R^2 = 1 - \frac{SSE}{SST},$

where SSE and SST is

$SST = {\bf Y}^T({\bf I}-\frac{1}{n}{\bf 1 1}^T){\bf Y},$

$SSE = ({\bf Y}-{\bf X}{\hat{\beta}})^T({\bf Y}-{\bf X}{\bf \hat{\beta}}).$

These values are presented in a "summary-table" in a similar way as the summary corresponding to `lm`. The following shows the summary of the regression using `mylm` and `lm` respectively. 

```{r, echo=FALSE}
summary(model1)
summary(model1b)
```

## c)

In the code chunk below, the code for the residual plot is shown along with the plot for `model1`. The raw residuals are the difference between the observed responses $Y$ and the estimated response $\hat Y$. For this specific dataset, these values are respectively the observed and estimated values for `wages`. The formula for the residuals is  

$$\hat\epsilon = Y-\hat Y, $$ where 

$$\hat Y = X\hat \beta. $$

```{r, echo=FALSE}
library(ggplot2)
plot.mylm <- function(object){
  # Code here is used when plot(object) is used on objects of class "mylm"
  y_hat <- object$X %*% object$coefficients 
  y_true=object$y
  residuals=y_true-y_hat
  data <- data.frame(y_true,y_hat, residuals)
  # Code here is used when plot(object) is used on objects of class "mylm"
  p <- ggplot(data=data, aes(x=y_hat,y=residuals))+geom_point() +
          ggtitle("Residuals vs. fitted values") + stat_smooth(method="loess")
  p
}
model1 <- mylm(wages ~ education, data = SLID)
model1b <- lm(wages ~ education, data = SLID)
plot(model1)
#plot(model1b)
```

In the mylm-residual plot for the SLID dataset, it seems like the residuals are not homoscedastic, meaning that the
variance seem to increase with higher values on the x-axis. However, there are fewer data points for lower values of the fitted values, so this tendency might be due to randomness. We also want the residuals to be symmetrically distributed around zero, which is not the case. This implies that the residuals might be correlated and we might want to try other regression models as the model assumptions is not preserved.

To ensure the function was coded correctly, the residual plot was compared to the plot from the plotting function for lm-objects. 

## d)
The residual sum of squares, $SSE$, can be found by
$$SSE = (\textbf{Y}-\textbf{X} \mathbf{\beta})^T(\textbf{Y}-\textbf{X}\mathbf{\beta}), $$ and for the model at hand, $SSE = 223694.3$. For a model defined by a design matrix of dimension $n\times p$ the degrees of freedom, $df$, is given by $df = n-p$. In this case $df = 3987-2 = 3985$. 

The total sum of squares, $SST$, is defined as $$SST = \textbf{Y}^T (\textbf{I}-\frac{1}{n}\textbf{1}\textbf{1}^T) \textbf{Y}.$$ For the model in this task $SST = 246790.5$. In order to test the significance of the regression, a $\chi^2$-test can be used. The test statistic is then given by $\chi^2 = (\textrm{numerator degrees of freedom})F$, where $F$ is the test statistic for a $F$-test and the numerator degrees of freedom is equal to $p-1 = k$. Hence, the test statistic for the $\chi^2$-test is $$\chi^2 = \frac{(SST-SSE)}{\frac{SSE}{n-p}}.$$ The value of the test statistic is $\chi^2=411.4471$.To test for the significance of the regression one can find the p-value associated with this test statistic. The p-value in this case is $1.77\cdot10^{-91}$. With a p-value this small, it is reasonable to conclude that the regression is significant.

When testing one regression parameter, the $F$-statistic is equal to the squared of the $T$-statistic. This is the case for single linear regression. The $\chi^2$-distribution is the asymptotic distribution of the $F$-distribution, and the normal distribution is the asymptotic distribution of the $T$-distribution. Therefore the $\chi^2$-statistic should be equal to the square of the $z$-statistic for a simple linear regression. The $z$-statistic for the regression coefficient in the model at hand is $z = 20.284$, and $z^2 = 411.4407$. The difference between the test statistics is probably due to rounding errors. The p-values for the two test statistics is also equal. The critical value for the $\chi^2$ test is found to be $3.841$ for significance level $\alpha = 0.05$ and the critical value for the $z$-test is $1.96$ for $\alpha = 0.025$. The critical value for $\chi^2$ is equal to the critical value of $z$ squared. 
 
## e)
 
The coefficient of determination, $R^2$, is defined as $$R^2 = \frac{SST-SSE}{SST},$$ and it is a measure of how much of the variance that is explained by the model. The maximum value of $R^2$ is 1, and a value close to 1 is preferred. For the current model the summary function now returns the value of $R^2$, as seen in the summary print-out in problem 2b).


# Part 3

## a)
The matemathical formulas presented in part 2 works for multiple linear regression as well. The printout of the regression of `wages` with predictors `education` and `age` using the `mylm` function and the `lm` function are

```{r, echo = FALSE}
model2 <- mylm(wages ~ education + age, data = SLID)
model2b <- lm(wages ~ education + age, data = SLID)

print(model2)
print(model2b)
```

## b)

The summaries for this regression using `lm` and `mylm` including standard deviation and z-test are shown below.

```{r, echo= FALSE}
summary(model2)
summary(model2b)
```

The estimated coefficient of predictor $i$ can be interpreted as the value by which the wages will increase when predictor $i$ increases by one and the other predictors are kept constant.

## c) 

Below, three different models are fitted using the `mylm`-package with `wages` as response. The first is with `age` as the only covariate, the second is with `language` as the only covariate, and the third is with both `age` and `language` in a multiple regression.

```{r, echo=FALSE}
model3 = mylm(wages ~ age, data = SLID)
model4 = mylm(wages ~ education, data =SLID)
model5 = mylm(wages ~education + age, data = SLID)
summary(model3)
summary(model4)
summary(model5)
```

We observe that the coefficients for `age` in the simple and multiple model is respectively $0.23$ and $0.26$. For education, the values are $0.79$ and $0.90$. The values differ slightly. This is because the matrix $\bf{X^TX}$ is not diagonal and the design matrix is not orthogonal. This means that in our case, the estimated coefficients are not independent of each other. The estimated coefficient for `age` will affect the value for the estimated coefficient for `education`. This is also what was found in problem 1, where we found a small correlation between `age` and `education`.

# Part 4

In this part we test our `mylm`-function on three different models. To ensure our package is correct, all values are compared to values generated by the `lm`-function.

**Model 1** 

In this model, we estimate `wages` as a function of `sex`, `age`, `language` and `education^2`. The summary and residual plot is included below. 

```{r, echo=FALSE}
model1 <- mylm(wages ~ sex + age + language + I(education^2), data = SLID)
summary(model1)
plot(model1)
```

The intercept for this model is -1.88. This means that the average wage of a person which is female and of age 0, has 0 years of education squared and is english-speaking, is -1.88 (these values does not make sense, but it is how we interpret the intercept). The interpretation of the coefficient `sexMale` is that the average increase in wage for being male compared to being female is 3.41. This means that if the observation is male, the wage will increase by 3.41 compared to a female observation with the rest of the covariates identical to the male observation. For the `age`-coefficient, the coefficient implies that if you hold the other covariates constant and increase `age` by one year, the wage will increase by 0.25. For the `language`-covariate, the interpretations of the coefficients are that an french or other-language-speaking observation will have a decrease of respectively 0.08 and 0.13 compared to an english-speaking observation. However, these covariates are not significant. The interpretation of the `education^2`-parameter is that if education^2 increases by one unit, the estimated wage will increase by 0.03.

From the p-values, we see that all covariates execept `language` are significant. The $R^2$ value is the fraction of variance that is explained by the regression. For this regression, we see from the summary above that that $R^2=0.30$. From the residual plot, there is a tendency of increased variance of the residuals as the fitted values increases. Thus, we might want to consider another model as we do not have homoscedastic residuals which is an assumption for the linear regression.

Since we have many covariates and based on the both significance of the coefficients and the residual plot, a change to make this model better can be to try to assess a model without the `language`-covariate. 

**Model 2**

Here we estimate `wages` as a function of `language` and `age`, in addition to an interaction term between the two covariates. 

```{r, echo=FALSE}
model2 <- mylm(wages ~ language + age + language*age, data = SLID)
summary(model2)
plot(model2)
```

Here, the interpretation of the intercept is that for an observation of age 0 and english-speaking, the estimated wage is 6.55. The interpretation of the language coefficient is the same as in `model1`, except that the values differ. The interpretation for the age-coefficient is that if all other covariates are kept constant, the wage increases by 0.24 when the age increases by one year. The interaction effect between age and language can be interpreted as an adjustment on the effect of age if the observation is not english speaking: If the observation is french speaking, the wage will additionally decrease by 0.08 for each year. Simirlarly, if the observation is other-language speaking, the french-age interaction will be zero, but the interaction between other-language and age will give an adjustment on -0.04 if the year increases by one and the rest of the covariates are kept constant.

For this model, $R^2=0.13$ which is lower than for `model1`. Since the number of estimated covariates is the same for the two models, this means that a higher fraction of the variance is explained by factors we have not included in this model. Also, only intercept and `age` are significant on significance level $0.001$. The interaction between `languageFrench` and `age`is significant on level 0.05. Since a model with interaction but no main effect is hard to interpret, we might want to consider a model without `language` but also include `education` which has turned out to be significant previously. 

From the residual plot, we see that the residuals are not homoscedastic, also indicating that we might want to try a different model.

**Model 3**

Now, we remove the intercept from our model and estimate `wages` as a function of education only.

```{r, echo=FALSE}
model3 <- mylm(wages ~ -1 +education, data = SLID)
summary(model3)
plot(model3)
```

Now, the $R^2$-value do not make sense, but from the p-value in the summary, we see that the regression is significant. The interpretation of the parameters in this model is that for an observation with zero years of education, the wage is zero, and if year increases by one, the wage increase with 1.15.

In the residual plot, we see that the variance is not homoscedastic and not symmetrically distributed around zero. For lower values, there are fewer data points and less symmetry. 

A small change that could make the model better is to try to include `age` and intercept, which have seemed to be significant in the earlier models.
