--- 
title: 'TMA4315: Compulsory exercise 1 (title)'
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
date: '`r format(Sys.time(), ''%d.%m.%Y'')`'
subtitle: 'Group 0: Name1, Name2 (subtitle)'


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Part 1

## a)

The following shows some diagnostic polts based on the data set from Canada, containing information about wages, education, age, sex and language. 

```{r, echo = FALSE}
library(GGally)
#install.packages("car")
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]
ggpairs(SLID)
```

The properties that seem to be most correlated with the wages is age and education, with numerical correlation value of 0.36 and 0.306 respectively. On the scatter plots of wages versus age and wage versus education, it can be seen that among those with highest wages there are few with low age and low education. 

On the other hand, the wage level does not seem to be related to the language of the observed people. It is also little correlation between wages and sex, but the average wage is slightly higher for the males than for the females. 

If we want to perform a multiple linear regression analysis to predict wages based on some of the other variables we have to assume that relationship between wages and the variebles is in fact linear. Also, the residuals are assumed to be normally distributed and homoscedastic, and the explanatory covariates are assumed to be uncorrelated. 

#Part 2

## a) 

```{r, echo = FALSE}
mylm <- function(formula, data = list(), contrasts = NULL, ...){
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")
  n = nrow(X)
  p = ncol(X)
  k = p-1

  # Add code here to calculate coefficients, residuals, fitted values, etc...
  # and store the results in the list est
  beta_hat = solve(t(X)%*%X)%*%t(X)%*%y
  sigma_sq_hat = (1/(n-p))*t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  cov_beta_hat = drop(sigma_sq_hat)*solve(t(X)%*%X)
  est <- list(terms = terms, model = mf)
  est$coefficients <- beta_hat
  est$cov <- cov_beta_hat

  # Store call and formula used
  est$call <- match.call()
  est$formula <- formula

  df = n-p
  SSE = t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  SST = t(y)%*%(diag(n)-drop(1/n)*rep(1, n)%*%t(rep(1, n)))%*%y
  R2 = 1-SSE/SST
  X2 = ((SST-SSE))/(SSE/(n-p))
  pX2 = pchisq(X2, k)
  # What is the relationship between the X2-statistic and the z-statistic?
  # Kanskje "Testing one regression coefficient" i modul 2
  # Find the critical value for both tests
  vec = c(SSE, SST, R2, X2, pX2, n, p, df)
  est$vec = vec

  # Set class name. This is very important!
  class(est) <- 'mylm'
  est$X <- X
  est$y <- y
  # Return the object with all results
  return(est)
}

print.mylm <- function(object, ...){
  # Code here is used when print(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("Coefficients: ")
  for (i in 1:(length(names))){
    cat("\n")
    cat(names[i])
    cat(": ")
    cat(object$coefficients[i])
  }
  cat("\n")
}

summary.mylm <- function(object, ...){
  # Code here is used when summary(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("R squared: ")
  cat(object$vec[3])
  cat("\n")
  cat("Coefficients:\n")
  SD = vector(length = object$vec[7])
  for(i in 1:object$vec[7]){
    SD[i] = sqrt(object$cov[i,i])
  }
  Estimates = object$coefficients
  ztest = Estimates/SD
  pvalue = 2*pnorm(-abs(ztest))
  table = data.frame(Estimates, SD, ztest, pvalue)
  rownames = vector(length = object$vec[7])
  for(i in 1:length(names)){
    rownames[i] = names[i]
  }
  row.names(table) = rownames
  format(table, digits = 4)
}


```

In order to estimate the regression coefficients we have used

${\hat{\beta}} = (X^TX)^{-1}X^TY$

The following prints gives the estimated intercept and coefficient from the linear regression with wages as response and education as predictor using the mylm and lm functions respectively.

```{r, echo=FALSE}
model1 <- mylm(wages ~ education, data = SLID)
print(model1)

model1b <- lm(wages ~ education, data = SLID)
print(model1b)

```

We observe that the two functions calculates equal values, indicating that our function works correctly for these estimations. 

# b)

We have estimated the variance as

$\hat{\sigma}^2 = \frac{1}{n-p} (Y -X\hat{\beta})^T(Y-X\hat{\beta})$

and further found the covariance matrix as

$Cov = \hat{\sigma}^2 X^T X$

The standard deviation for the estimated coefficients are set to the corresponding diagonal element of the covariance matrix. 

The z-statistic has the value

$z = \frac{\hat{\beta} - \beta_0}{SD} = \frac{\hat{\beta}}{SD}$

where $\beta_0$ is the beta-value of the the null hypothesis, in this case $\beta_0 = 0$. 

The p-value is calculated as $2 \times pnorm(-|z|)$, where "pnorm" is a function in r which returns the p-value corresponding to its input. We are interested in the p-value corresponding to the two-sided test, so therefore we are multiplying the value by two. 


```{r, echo=FALSE}
summary(model1)
summary(model1b)
```

# Problem 1c)
In the code chunk below, the code for the residual plot is shown along with the plot for `model1`. The raw residuals is the difference between the observed responses $y$ and the estimated response $\hat y$. For this specific dataset, these values is respectively the observed and estimated values for `wages`. The formula for the the residuals is  

$$\hat\epsilon = y-\hat y $$.

```{r, echo=FALSE}
library(ggplot2)
plot.mylm <- function(object){
  # Code here is used when plot(object) is used on objects of class "mylm"
  y_hat <- object$X %*% object$coefficients 
  y_true=object$y
  residuals=y_true-y_hat
  data <- data.frame(y_true,y_hat, residuals)
  # Code here is used when plot(object) is used on objects of class "mylm"
  p <- ggplot(data=data, aes(x=y_hat,y=residuals))+geom_point() +
          ggtitle("Residuals vs. fitted values") + stat_smooth(method="loess")
  p
}
model1 <- mylm(wages ~ education, data = SLID)
model1b <- lm(wages ~ education, data = SLID)
plot(model1)
plot(model1b)
```

In the mylm-residual plot for the SLID dataset, it seems that the residuals are not homoscedastic, meaning that the
variances seem to increase with higher values on the x-axis. However, there are fewer data points for lower values of the fitted values, so this tendency might be due to randomness. We also want the residuals to be symmetrically distributed around zero, which is not the case. This implies that the residuals might be correlated and we might want to try another regression model as the model assumptions is not preserved.

To ensure the function was coded correctly, the residual plots was compared to the plot from the plotting function for lm-objects. 

#Part 4

In this part we test our `mylm`-function on three different models. To ensure our package is correct, all values are compared to values generated by the `lm`-function.

**Model 1** 

In this model, we estimate `wages` as a function of `sex`, `age`, `language` and `education^2`. The summary and residual plot is included below. 

```{r, echo=FALSE}
model1 <- mylm(wages ~ sex + age + language + I(education^2), data = SLID)
summary(model1)
plot(model1)
```
The $R^2$ value is the fraction of variance that is explained by the regression. For this regression, we see from the summary above that that $R^2=0.30$. From the p-values, we see that all covariates execept `language` are significant. A change to make this model better might be to exclude `language`.

**Model 2**

Here we estimate `wages` as a function of `language` and `age`, in addition to an interaction term between the covariates. 

```{r, echo=FALSE}
model2 <- mylm(wages ~ language + age + language*age, data = SLID)
summary(model2)
plot(model2)
```

For this model, $R^2=0.13$ which is lower than for `model1`. Since the number of estimated covariates is the same for the two models, this means that a higher fraction of the variance is explained by factors we have not included in this model. Also, only intercept and `age` are significant on significance level $0.001$. The interaction between `languageFrench` and `age`is significant on level 0.05. Since a model with interaction but no main effect is hard to interpret, we might want to consider a model without `language` but also include `education` which has turned out to be significant previously. 

From the residual plot, we see that the residuals are not homoscedastic, indicating that we might want to try a different model.

**Model 3**
Now, we remove the intercept from our model and estimate `wages` as a function of education only.
```{r, echo=FALSE}
model3 <- mylm(wages ~ -1 +education, data = SLID)
summary(model3)
plot(model3)
```

Now, the $R^2$-value do not make sense, but from the p.value in the summary, we see that the regression is significant. In the residual plot, we see that the variance is homoscedastic for values over 15 of estimated `wages`, but they are not quite symmetrically distributed around zero. For lower values, there seems to be an increasing variation, but we note that there are also fewer data points, so this might be due to randomness. 

A small change that could make the model better is to include age, which have seemed to be significant in the earlier models.