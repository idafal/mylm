--- 
title: 'TMA4315: Compulsory exercise 1 (title)'
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group 0: Name1, Name2 (subtitle)'
---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Introduction

In this compulsary exercise we are going to make an r package, called "mylm", with some of the same functionality as found in the already existing r package `lm`. We are going to test our implementations on the

# Part 1

## a)

The aim of part 1 is to get an overview of the dataset before making the linear regressions. The following diagram shows some diagnostic polts based on the data set from Canada in the car-library, containing information about `wages`, `education`, `age`, `sex` and `language`. 

```{r, echo = FALSE}
library(GGally)
#install.packages("car")
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID), ]
ggpairs(SLID)
```

In the later exercises we are going to perform regression using `wages` as response. The properties that seem to be most correlated with the `wages` is `age` and `education`, with numerical correlation value of 0.36 and 0.306 respectively. On the scatter plots of `wages` versus `age` and `wages` versus `education`, it can be seen that among those with highest wages there are few with low age and low education. 

On the other hand, the wage level does not seem to be related to the `language` of the observed people. It also shows little correlation with `sex`, but the average wage is slightly higher for the males than for the females. 

If we want to perform a multiple linear regression analysis to predict wages based on some of the other variables we have to assume that relationship between wages and the variebles is in fact linear. Also, the residuals are assumed to be normally distributed and homoscedastic, and the explanatory covariates are assumed to be uncorrelated.

Based on the diagnostic plots, none of the explanatory variables seem to be highly correlated. However, for instance `education` and `age` correlates slightly, with coefficient of correlation of -0.106. 

#Part 2

## a) 

```{r, echo = FALSE}
mylm <- function(formula, data = list(), contrasts = NULL, ...){
  # Extract model matrix & responses
  mf <- model.frame(formula = formula, data = data)
  X  <- model.matrix(attr(mf, "terms"), data = mf, contrasts.arg = contrasts)
  y  <- model.response(mf)
  terms <- attr(mf, "terms")
  n = nrow(X)
  p = ncol(X)
  k = p-1

  # Add code here to calculate coefficients, residuals, fitted values, etc...
  # and store the results in the list est
  beta_hat = solve(t(X)%*%X)%*%t(X)%*%y
  sigma_sq_hat = (1/(n-p))*t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  cov_beta_hat = drop(sigma_sq_hat)*solve(t(X)%*%X)
  est <- list(terms = terms, model = mf)
  est$coefficients <- beta_hat
  est$cov <- cov_beta_hat
  
  est$X <- X
  est$y <- y

  # Store call and formula used
  est$call <- match.call()
  est$formula <- formula

  df = n-p
  SSE = t(y-X%*%beta_hat)%*%(y-X%*%beta_hat)
  SST = t(y)%*%(diag(n)-drop(1/n)*rep(1, n)%*%t(rep(1, n)))%*%y
  R2 = 1-SSE/SST
  X2 = ((SST-SSE))/(SSE/(n-p))
  pX2 = pchisq(X2, k)
  # What is the relationship between the X2-statistic and the z-statistic?
  # Kanskje "Testing one regression coefficient" i modul 2
  # Find the critical value for both tests
  vec = c(SSE, SST, R2, X2, pX2, n, p, df)
  est$vec = vec

  # Set class name. This is very important!
  class(est) <- 'mylm'

  # Return the object with all results
  return(est)
}

print.mylm <- function(object, ...){
  # Code here is used when print(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("Coefficients: ")
  for (i in 1:(length(names))){
    cat("\n")
    cat(names[i])
    cat(": ")
    cat(object$coefficients[i])
  }
  cat("\n")
}

summary.mylm <- function(object, ...){
  # Code here is used when summary(object) is used on objects of class "mylm"
  # Useful functions include cat, print.default and format
  names <- colnames(object$cov)
  cat("R squared: ")
  cat(object$vec[3])
  cat("\n")
  cat("Coefficients:\n")
  SD = vector(length = object$vec[7])
  for(i in 1:object$vec[7]){
    SD[i] = sqrt(object$cov[i,i])
  }
  Estimates = object$coefficients
  ztest = Estimates/SD
  pvalue = 2*pnorm(-abs(ztest))
  table = data.frame(Estimates, SD, ztest, pvalue)
  rownames = vector(length = object$vec[7])
  for(i in 1:length(names)){
    rownames[i] = names[i]
  }
  row.names(table) = rownames
  format(table, digits = 4)
}


```

In order to estimate the regression coefficients, $\mathbf{\hat{\beta}}$ we have used the matrix formula

${\bf{\hat{\beta}}} = (X^TX)^{-1}X^TY$

The following printouts gives the estimated intercept and regression coefficient from the linear regression with `wages` as response and `education` as predictor using `mylm` and `lm` respectively.

```{r, echo=FALSE}
model1 <- mylm(wages ~ education, data = SLID)
print(model1)

model1b <- lm(wages ~ education, data = SLID)
print(model1b)

```

This shows that the two functions calculates equal values, indicating that our function works as it was ment to in this case. 

# b)

We have estimated the variance as

$\hat{\sigma}^2 = \frac{1}{n-p} (Y -X\hat{\beta})^T(Y-X\hat{\beta})$

and further found the covariance matrix as

$Cov = \hat{\sigma}^2 X^T X$

The standard deviation for the estimated coefficients are set to the corresponding diagonal element of the covariance matrix. 

The z-statistic for the z-test has the value

$z = \frac{\hat{\beta} - \beta_0}{SD} = \frac{\hat{\beta}}{SD}$

where $\beta_0$ is the beta-value of the the null hypothesis, in this case $\beta_0 = 0$. 

The p-value is calculated as $2 \times pnorm(-|z|)$, where "pnorm" is a function in r which returns the p-value corresponding to the value of its input statistic. We are interested in the p-value corresponding to the two-sided test, so therefore we are multiplying the value by two. 

$R^2$ is calculated as 

$R^2 = 1 - \frac{SSE}{SST}$

where SSE and SST is

$SST = {\bf Y}^T({\bf I}-\frac{1}{n}{\bf 1 1}^T){\bf Y}$

$SSE = ({\bf Y}-{\bf X}{\hat{\beta}})^T({\bf Y}-{\bf X}{\bf \hat{\beta}})$

These values are presented in a "summary-table" in a similar way as the summary corresponding to `lm`. The following shows the summary of the regression using `mylm` and `lm` respectively. 

```{r, echo=FALSE}
summary(model1)
summary(model1b)
```

# c)

In the code chunk below, the code for the residual plot is shown along with the plot for `model1`. The raw residuals is the difference between the observed responses $y$ and the estimated response $\hat y$. For this specific dataset, these values is respectively the observed and estimated values for `wages`. The formula for the the residuals is  

$$\hat\epsilon = y-\hat y $$.

```{r, echo=FALSE}
library(ggplot2)
plot.mylm <- function(object){
  # Code here is used when plot(object) is used on objects of class "mylm"
  y_hat <- object$X %*% object$coefficients 
  y_true=object$y
  residuals=y_true-y_hat
  data <- data.frame(y_true,y_hat, residuals)
  # Code here is used when plot(object) is used on objects of class "mylm"
  p <- ggplot(data=data, aes(x=y_hat,y=residuals))+geom_point() +
          ggtitle("Residuals vs. fitted values") + stat_smooth(method="loess")
  p
}
model1 <- mylm(wages ~ education, data = SLID)
model1b <- lm(wages ~ education, data = SLID)
plot(model1)
#plot(model1b)
```

In the mylm-residual plot for the SLID dataset, it seems that the residuals are not homoscedastic, meaning that the
variances seem to increase with higher values on the x-axis. However, there are fewer data points for lower values of the fitted values, so this tendency might be due to randomness. We also want the residuals to be symmetrically distributed around zero, which is not the case. This implies that the residuals might be correlated and we might want to try another regression model as the model assumptions is not preserved.

To ensure the function was coded correctly, the residual plots was compared to the plot from the plotting function for lm-objects. 

# Part 3

## a)
The matemathical formulas presented in part 2 works for multiple linear regression as well. The printout of the regression of wages with predictors education and wage using the mylm function and the lm function are

```{r, echo = FALSE}
model2 <- mylm(wages ~ education + age, data = SLID)
model2b <- lm(wages ~ education + age, data = SLID)

print(model2)
print(model2b)

```

## b)

Thu summaries for this regression using lm and mylm including standard deviation and z-test are shown below.

```{r, echo= FALSE}
summary(model2)
summary(model2b)
```

The estimated coefficient of predictor $i$ can be interpreted as the value by which the wages will increase when predictor $i$ increases by one and the other predictors are kept constant.

## c) 

```{r, echo=FALSE}
model3 = mylm(wages ~ age, data = SLID)
```

# Part 4

In this part we test our `mylm`-function on three different models. To ensure our package is correct, all values are compared to values generated by the `lm`-function.

**Model 1** 

In this model, we estimate `wages` as a function of `sex`, `age`, `language` and `education^2`. The summary and residual plot is included below. 

```{r, echo=FALSE}
model1 <- mylm(wages ~ sex + age + language + I(education^2), data = SLID)
summary(model1)
plot(model1)
```

The $R^2$ value is the fraction of variance that is explained by the regression. For this regression, we see from the summary above that that $R^2=0.30$. From the p-values, we see that all covariates execept `language` are significant. A change to make this model better might be to exclude `language`.

**Model 2**

Here we estimate `wages` as a function of `language` and `age`, in addition to an interaction term between the covariates. 

```{r, echo=FALSE}
model2 <- mylm(wages ~ language + age + language*age, data = SLID)
summary(model2)
plot(model2)
```

For this model, $R^2=0.13$ which is lower than for `model1`. Since the number of estimated covariates is the same for the two models, this means that a higher fraction of the variance is explained by factors we have not included in this model. Also, only intercept and `age` are significant on significance level $0.001$. The interaction between `languageFrench` and `age`is significant on level 0.05. Since a model with interaction but no main effect is hard to interpret, we might want to consider a model without `language` but also include `education` which has turned out to be significant previously. 

From the residual plot, we see that the residuals are not homoscedastic, indicating that we might want to try a different model.

**Model 3**

Now, we remove the intercept from our model and estimate `wages` as a function of education only.

```{r, echo=FALSE}
model3 <- mylm(wages ~ -1 +education, data = SLID)
summary(model3)
plot(model3)
```

Now, the $R^2$-value do not make sense, but from the p.value in the summary, we see that the regression is significant. In the residual plot, we see that the variance is homoscedastic for values over 15 of estimated `wages`, but they are not quite symmetrically distributed around zero. For lower values, there seems to be an increasing variation, but we note that there are also fewer data points, so this might be due to randomness. 

A small change that could make the model better is to include `age`, which have seemed to be significant in the earlier models.
